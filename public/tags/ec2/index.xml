<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ec2 on My Tech Blog</title>
    <link>https://www.vishnu-tech.com/tags/ec2/index.xml</link>
    <description>Recent content in Ec2 on My Tech Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://www.vishnu-tech.com/tags/ec2/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Setup Rancher with HA &amp; running Kubernetes on top of it.</title>
      <link>https://www.vishnu-tech.com/blog/Setup-Rancher-with-HA/</link>
      <pubDate>Mon, 06 Mar 2017 19:43:25 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/Setup-Rancher-with-HA/</guid>
      <description>

&lt;p&gt;&lt;em&gt;In this blog, we are setting up a highly available Rancher cluster in AWS and running Kubernetes on top of it&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;why-rancher&#34;&gt;Why Rancher?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Rancher is a complete, open source platform for deploying and managing containers in production. It includes commercially-supported distributions of Kubernetes, Mesos, and Docker Swarm, making it easy to run containerized applications on any infrastructure.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;follow-the-below-steps&#34;&gt;Follow the below steps:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;FYI, from the below images, you may notice the ELB&amp;rsquo;s &amp;amp; ip&amp;rsquo;s, don&amp;rsquo;t worry ğŸ™„ğŸ™„ it&amp;rsquo;s a temporary cluster created for testing ğŸ˜›ğŸ˜œ&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Here I will be creating the AWS setup as below:&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-aws-setup.png&#34; alt=&#34;Rancher setup in AWS&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Create 3 instance for Rancher Server using the Community AMI&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-server.png&#34; alt=&#34;Rancher Server in AWS&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Create 3 instance for Rancher Host using the Community AMI&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-host.png&#34; alt=&#34;Rancher Host in AWS&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Create ELB for Rancher Server&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-elb.png&#34; alt=&#34;ELB for Rancher Server in AWS&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Put ELB Listeners as below:&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-elb-listeners.png&#34; alt=&#34;ELB Listeners for Rancher Server&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Put ELB Healthcheck as below:&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-elb-healthcheck.png&#34; alt=&#34;ELB Healthcheck for Rancher Server&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Created 4 security groups:&lt;/em&gt;
You can view more details from &lt;a href=&#34;https://docs.rancher.com/rancher/v1.5/en/installing-rancher/installing-server/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/aws-rancher-sg.png&#34; alt=&#34;Security for Rancher Server&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Once you setup the ELB, you need to enable the proxy protocol mode&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;$ aws elb create-load-balancer-policy \
     --load-balancer-name rancher-elb \  
     --policy-name rancher-elb-policy \
     --policy-type-name ProxyProtocolPolicyType \
     --policy-attributes AttributeName=ProxyProtocol,AttributeValue=true \
     --region eu-west-1

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;
$ aws elb set-load-balancer-policies-for-backend-server \
      --load-balancer-name rancher-elb \
      --instance-port 80 \
      --policy-names rancher-elb-policy  \
      --region eu-west-1

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;
$ aws elb set-load-balancer-policies-for-backend-server \
      --load-balancer-name rancher-elb \
      --instance-port 8080 \
      --policy-names rancher-elb-policy \
      --region eu-west-1

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Create a MySql RDS instance&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-mysql.png&#34; alt=&#34;Mysql RDS instance&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Once the Rancher Server is UP, Login to those servers and run the below command:&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
docker run -d --restart=unless-stopped -p 8080:8080 -p 9345:9345 \
       rancher/server:latest \
       --db-host &amp;lt;your rds endpoint&amp;gt; \
       --db-port 3306 --db-user &amp;lt;your db user&amp;gt; \
       --db-pass &amp;lt;your db password&amp;gt; --db-name &amp;lt;your db name&amp;gt; \
       --advertise-address &amp;lt;put your rancher private server ip&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Once the cluster is UP, you can authorize it with Github&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-access.png&#34; alt=&#34;Rancher access&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/rancher-oauth-github.png&#34; alt=&#34;Rancher github&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Once you authorized, you can add host like below:&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/add-custom-host.png&#34; alt=&#34;Rancher custom host&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can also add host by running the command on each host nodes like below:&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;sudo docker run -e CATTLE_AGENT_IP=&amp;quot;172.31.42.207&amp;quot;  \
    -d --privileged \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v /var/lib/rancher:/var/lib/rancher \
    rancher/agent:v1.2.1 http://rancher-elb-1978581874.eu-west-1.elb.amazonaws.com/v1/scripts/79081F08916A15D4F9F8:1483142400000:k2tRsgImqrdonHsdFYEtpI2ss

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can also bootup AWS instance if you want&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/add-aws-host.png&#34; alt=&#34;Rancher aws host&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can see once the host is UP&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/active-host.png&#34; alt=&#34;Rancher active host&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can choose different type of applications to install&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/catalog1.png&#34; alt=&#34;Rancher catalog1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/catalog2.png&#34; alt=&#34;Rancher catalog2&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can easily install Kubernetes from the catalog&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/k8s-template.png&#34; alt=&#34;Rancher Kubernetes&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can verify the installation by selecting the stack&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/starting-k8s.png&#34; alt=&#34;Rancher Kubernetes stack&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You can also list the containers to get more info&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/container-info.png&#34; alt=&#34;Rancher Kubernetes containers&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;you-can-get-more-info-from-here-http-rancher-com&#34;&gt;You can get more info from &lt;a href=&#34;http://rancher.com/&#34;&gt;here&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&#34;for-automation-you-check-this-ansible-playbook-https-github-com-galal-hussein-rancher-ansible&#34;&gt;For automation, you check this Ansible &lt;a href=&#34;https://github.com/galal-hussein/Rancher-Ansible&#34;&gt;playbook&lt;/a&gt;&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes with HA in AWS</title>
      <link>https://www.vishnu-tech.com/blog/Kubernetes-HA-in-AWS/</link>
      <pubDate>Tue, 28 Feb 2017 21:23:25 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/Kubernetes-HA-in-AWS/</guid>
      <description>

&lt;p&gt;&lt;em&gt;In this blog, we are setting up a highly available Kubernetes cluster in AWS using Kops&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I am creating our Kubernetes cluster in a private VPC.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I hope everyone knows how to create a VPC architecture in AWS, so I will be mainly focusing on K8s.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.vishnu-tech.com/images/aws-k8s.png&#34; alt=&#34;K8s setup in AWS&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;AWS:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Cli&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;VPC&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Public Subnets&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Private Subnets&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Route Tables&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Internet Gateway&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;NAT Gateway&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Route53 domain&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Kops:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You can easily download from &lt;a href=&#34;https://github.com/kubernetes/kops/releases&#34;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Kubectl:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You can easily install the package from &lt;a href=&#34;https://kubernetes.io/docs/user-guide/prereqs/&#34;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;AWS S3 bucket:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;You need to create a unique AWS S3 bucket to manage your clusters even after installation.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Kops keep track of the clusters that you have created, along with their configuration, the keys they are using etc. This information is stored in an S3 bucket.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;K8s HA setup:&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
kops create cluster \
    --node-count 3 \
    --zones us-east-1a,us-east-1b,us-east-1c \
    --master-zones us-east-1a,us-east-1b,us-east-1c \
    --dns-zone example.com \
    --node-size m4.large \
    --master-size m4.xlarge \
    --topology private \
    --networking weave \
    --vpc vpc-idxxxxx \
    --name k8s.example.com \
    --state s3://kops-state-k8s \
    --bastion

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--node-count:-&lt;/code&gt;  &lt;em&gt;Specify the number of nodes&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--zones:-&lt;/code&gt; &lt;em&gt;Run nodes in multiple zones&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--master-zones:-&lt;/code&gt; &lt;em&gt;Run with a HA master&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--dns-zone:-&lt;/code&gt; &lt;em&gt;Specify your AWS DNS zone&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-size:-&lt;/code&gt; &lt;em&gt;Specify our K8s nodes to a defined instance type&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--master-size:-&lt;/code&gt; &lt;em&gt;Specify our K8s masters to a defined instance type&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--topology:-&lt;/code&gt; &lt;em&gt;Specify our nodes to launch in private/public subnets in VPC&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--networking:-&lt;/code&gt; &lt;em&gt;Specify which overlay network to use&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--vpc:-&lt;/code&gt; &lt;em&gt;Specify your VPC&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--name:-&lt;/code&gt;  &lt;em&gt;Specify name of your cluster&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--state:-&lt;/code&gt; &lt;em&gt;Specify where to store the cluster state information&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--bastion:-&lt;/code&gt; &lt;em&gt;Jump host&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;you-can-edit-the-cluster-by-running-the-below-command&#34;&gt;You can edit the cluster by running the below command:&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
root# kops edit cluster &amp;lt;put your cluster name&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;change-the-vpc-subnet-configuration-and-you-can-update-the-cluster-by-running-the-below-command&#34;&gt;Change the VPC, Subnet configuration and you can update the cluster by running the below command&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
root# kops update cluster &amp;lt;put your cluster name&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;once-you-are-finalized-the-aws-setup-you-can-execute-it-by-running-the-below-command&#34;&gt;Once you are finalized the AWS setup, you can execute it by running the below command&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;
root# kops update cluster &amp;lt;put your cluster name&amp;gt; --yes

&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Once your cluster is up, you can play around with &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-cheatsheet/&#34;&gt;Kubectl&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Docker swarm production setup</title>
      <link>https://www.vishnu-tech.com/blog/docker-swarm-production-setup/</link>
      <pubDate>Wed, 30 Nov 2016 19:08:10 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/docker-swarm-production-setup/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Docker Swarm is a cluster of docker engines.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Docker Swarm serves the standard Docker API, any tool that already communicates with a Docker daemon can use Swarm to transparently scale to multiple hosts.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Swarm mode is native in Docker 1.12 RC&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;You can orchestrate service when it&amp;rsquo;s in Swarm mode.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We are creating 3 swarm manager and 3 worker nodes for high availability in AWS.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;_You can view the full video from_Â &lt;strong&gt;&lt;a href=&#34;https://asciinema.org/a/94625&#34;&gt;here&lt;/a&gt;Â &lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ansible playbook for WhereHows (A Data Discovery and Lineage Portal)</title>
      <link>https://www.vishnu-tech.com/blog/ansible-playbook-for-wherehows-a-data-discovery-and-lineage-portal/</link>
      <pubDate>Tue, 18 Oct 2016 14:17:34 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/ansible-playbook-for-wherehows-a-data-discovery-and-lineage-portal/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/linkedin/WhereHows&#34;&gt;WhereHows&lt;/a&gt;, a data discovery and lineage portal. At LinkedIn, WhereHows integrates with all of their data processing environments and extracts coarse and fine grain metadata from them. Then, it surfaces this information through two interfaces:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A web application that enables navigation, search, lineage visualization, annotation, discussion, and community participation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An &lt;a href=&#34;https://github.com/linkedin/wherehows/wiki/Backend-API&#34;&gt;API&lt;/a&gt; endpoint that empowers automation of other data processes and applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://www.vishnu-tech.com/wp-content/uploads/2016/10/wherehows.png&#34;&gt;&lt;img src=&#34;https://www.vishnu-tech.com/wp-content/uploads/2016/10/wherehows.png&#34; alt=&#34;vu&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here I am creating an Ansible playbook to play around with WhereHows.&lt;/p&gt;

&lt;p&gt;You can find the playbook from &lt;a href=&#34;https://github.com/vishnudxb/ansible-wherehows&#34;&gt;here.&lt;/a&gt;Â &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TechNewsLetter Vol:15</title>
      <link>https://www.vishnu-tech.com/blog/technewsletter-vol15/</link>
      <pubDate>Wed, 31 Aug 2016 15:17:19 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/technewsletter-vol15/</guid>
      <description>&lt;p&gt;Sharing some interesting links to keep you busy!!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://thehackernews.com/2016/08/dropbox-data-breach.html&#34;&gt;Dropbox Hacked â€” More Than 68 Million Account Details Leaked Online&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://techcrunch.com/2016/08/30/dropbox-employees-password-reuse-led-to-theft-of-60m-user-credentials/amp/&#34;&gt;Dropbox employeeâ€™s password reuse led to theft of 60M+ user credentials&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://engineeringblog.yelp.com/2016/08/undebt-how-we-refactored-3-million-lines-of-code.html&#34;&gt;Undebt: How We Refactored 3 Million Lines of Code&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.zorinaq.com/nginx-resolver-vulns/&#34;&gt;Nginx resolver vulnerabilities allow cache poisoning attack&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://use-the-index-luke.com/blog/2016-07-29/on-ubers-choice-of-databases&#34;&gt;On Uberâ€™s Choice of Databases&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.kennethreitz.org/essays/on-cybersecurity-and-being-targeted&#34;&gt;On Cybersecurity and Being Targeted&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/dschep/ntfy&#34;&gt;A utility for sending notifications, on demand and when commands finish&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=12376596&#34;&gt;How do you handle DDoS attacks?&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/results?search_query=%22Big+Data+Day+LA+2016%22&#34;&gt;Videos from Big Data Day LA 2016&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://techblog.netflix.com/2016/08/building-fastcom.html&#34;&gt;Building fast.com&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/eon01/NodeSS&#34;&gt;nodeSS : Node.js Security Scanner&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/forking-docker-daniel-riek?trk=hp-feed-article-title-like&#34;&gt;Forking Docker Not&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/mozilla-tech/promoting-security-best-practices-with-observatory-7b164a190425#.neshhjqht&#34;&gt;Promoting Security Best Practices with Observatory&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/pulse/4-use-cases-insights-regarding-kubernetes-namespaces-van-velzen?trk=hp-feed-article-title-like&#34;&gt;4 use cases and insights regarding Kubernetes namespaces&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://thehackerblog.com/floating-domains-taking-over-20k-digitalocean-domains-via-a-lax-domain-import-system/index.html&#34;&gt;Floating Domains â€“ Taking Over 20K DigitalOcean Domains via a Lax Domain Import System&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://polynome.co/infosec/inversoft/elasticsearch/linode/penetration-testing/2016/08/16/hack-that-inversoft.html&#34;&gt;HackedThat: Breaking in to a hardened server via the back door&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.ilikebigbits.com/blog/2016/8/28/designing-a-fast-hash-table&#34;&gt;Designing a fast Hash Table&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ansible.com/blog/ansible-openshift-enterprise-container-platform&#34;&gt;Automating the provisioning and configuration of Redhat Mobile Application platform&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.slideshare.net/geshan/embrace-chatops-stop-installing-deployment-software-larcon-eu-2016&#34;&gt;Embrace chatops, stop installing deployment software&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/aws/powerful-aws-platform-features-now-for-containers/&#34;&gt;Powerful AWS Platform Features, Now for Containers&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/&#34;&gt;AWS Application Load Balancer&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/aws/aws-week-in-review&#34;&gt;The files in this GitHub Repo are used to produce the AWS Week in Review&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/bbc/chaos-lambda&#34;&gt;Randomly terminate ASG instances during business hours&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/stevenharradine/checkall&#34;&gt;Runs commands against every box within aws&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/DevOpsDave/ssh-everywhere&#34;&gt;Integrates ssh and tmux with aws cli to create tmux sessions that open a pane for each aws instance.&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.kubernetes.io/2016/08/create-couchbase-cluster-using-kubernetes.html&#34;&gt;Create a Couchbase cluster using Kubernetes&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Yelp/git-code-debt&#34;&gt;A dashboard for monitoring code debt in a git repository.&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/EntilZha/spot-price-reporter&#34;&gt;Fetch and plot AWS spot pricing history&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/cosiner/socker&#34;&gt;Socker is a library for Go to simplify the use of SSH&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/GrappigPanda/Olivia&#34;&gt;Go: A distributed, in-memory key-value storage.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/iamduo/workq&#34;&gt;Job server in Go&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.docker.com/2016/08/securing-enterprise-software-supply-chain-using-docker/&#34;&gt;Securing The Enterprise Software Supply Chain Using Docker&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://lostechies.com/gabrielschenker/2016/08/14/containers-clean-up-your-house/&#34;&gt;Containers â€“ Clean up your House&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://channel9.msdn.com/Shows/msftazure/Run-PowerShell-Natively-on-Linux-with-Docker&#34;&gt;Run PowerShell Natively on Linux with Docker&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.kendrickcoleman.com/index.php/Tech-Blog/how-to-use-volume-drivers-and-storage-with-new-docker-service-command.html&#34;&gt;How to Use Volume Drivers and Storage with New Docker Service Command&lt;/a&gt;Â Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.infoq.com/news/2016/08/docker-service-load-balancing&#34;&gt;Improved Options for Service Load Balancing in Docker 1.12.0&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.sqreen.io/one-easy-way-to-inject-malicious-code-in-any-node-js-application/&#34;&gt;One easy way to inject malicious code in any Node.js application&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.pivotal.io/pivotal/products/new-single-multi-node-sandboxes-for-pivotal-hdb-apache-hawq&#34;&gt;Docker-based sandbox (both single and multi-node) for Apache HAWQ &lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@allingeek/we-do-rest-is-not-enough-7fa2a683e2f4#.7eiyaabb6&#34;&gt;â€œWe do RESTâ€ is not Enough&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://dzone.com/articles/building-a-remote-caching-system&#34;&gt;Building a Remote Caching System&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way&#34;&gt;Bootstrap Kubernetes the hard way. &lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://githubengineering.com/context-aware-mysql-pools-via-haproxy/&#34;&gt;Context aware MySQL pools via HAProxy&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.codeship.com/autoscaling-purpose-strategies/&#34;&gt;Autoscaling: Its Purpose and Strategies&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://coreos.com/blog/fetching-and-running-docker-container-images-with-rkt.html&#34;&gt;Fetching and running docker container images with rkt&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.madewithtea.com/processing-tweets-with-kafka-streams.html&#34;&gt;Processing Tweets with Kafka Streams&lt;/a&gt;Â &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://inventous.com/database-scaling-mongodb/&#34;&gt;Database Scaling (Sharding) with MongoDB&lt;/a&gt;Â &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ansible playbooks to create private docker registry in AWS</title>
      <link>https://www.vishnu-tech.com/blog/ansible-playbooks-to-create-private-docker-registry-in-aws/</link>
      <pubDate>Fri, 19 Aug 2016 09:23:25 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/ansible-playbooks-to-create-private-docker-registry-in-aws/</guid>
      <description>

&lt;p&gt;Creating a private docker registry in AWS using Ansible playbooks. You can find the playbooks from &lt;a href=&#34;https://github.com/vishnudxb/ansible-docker-registry&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;variables-in-playbooks&#34;&gt;Variables in playbooks&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;_server_name: &amp;ldquo;docker-registry.vishnudxb.me&amp;rdquo; _&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;_ssl_bundle_crt: &amp;ldquo;/etc/nginx/ssl/ssl-bundle.crt&amp;rdquo; _&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;_ssl_crt&lt;em&gt;key: &amp;ldquo;/etc/nginx/ssl/ssl-cert.key&amp;rdquo;&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;aws-instance-tag-name&#34;&gt;AWS instance tag name&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The playbook will identified the instance using the tag name ie docker-registry&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;to-do&#34;&gt;To-Do&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Authorization for users inside the registry.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Running Openstack with LXC as hypervisor using Terraform</title>
      <link>https://www.vishnu-tech.com/blog/running-openstack-with-lxc-as-hypervisor-using-terraform/</link>
      <pubDate>Tue, 24 May 2016 18:35:19 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/running-openstack-with-lxc-as-hypervisor-using-terraform/</guid>
      <description>&lt;p&gt;Hi Guys,&lt;/p&gt;

&lt;p&gt;Today I got some time to play around with Terraform &amp;amp; Openstack. If you want a test Openstack environment with LXC in AWS, you can use this &lt;a href=&#34;https://github.com/vishnudxb/terraform-openstack-lxc&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It creates a new VPC environment in AWS in us-east-1 region and boot up an Ubuntu instance in the newly created VPC environment and install Openstack with LXC as hypervisor.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High Availability @ Load Balancing Layer-HAProxy / ELB</title>
      <link>https://www.vishnu-tech.com/blog/high-availability-load-balancing-layer-haproxy-elb/</link>
      <pubDate>Mon, 01 Apr 2013 07:23:25 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/high-availability-load-balancing-layer-haproxy-elb/</guid>
      <description>

&lt;p&gt;Architecting High Availability at the Load Balancing layer is one of the important aspects in the web scale systems in AWS. We can follow multiple strategies for achieving the same. I am listing some of the designs for achieving the same.&lt;/p&gt;

&lt;h1 id=&#34;pattern-1-route-53-dns-rr-haproxy&#34;&gt;&lt;em&gt;Pattern 1: Route 53 DNS RR + HAProxy&lt;/em&gt;&lt;/h1&gt;

&lt;p&gt;Route53 is a Managed DNS service provided by Amazon Web Services. Route 53 supports Round robin and weighted algorithms. If the Route53 DNS server has several entries for a given hostname, it will return all of them in a rotating order. This way, various users will see different addresses for the same name and will be able to reach different EC2 instances in LB Tier.&lt;/p&gt;

&lt;p&gt;$ host -t a HAProxyTestXYZ.com&lt;/p&gt;

&lt;p&gt;HAProxyTestXYZ.com. has address &lt;strong&gt;50.19.82.183 (Primary EIP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;HAProxyTestXYZ.com. has address &lt;strong&gt;23.23.174.254 (Secondary EIP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/1.jpg&#34;&gt;&lt;img src=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/1.jpg&#34; alt=&#34;1&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Example, if we attach the Elastic IPâ€™s of 2 HAProxy EC2 instances under the Route 53, both the IPâ€™s are sent to the user browsers by the Route 53 DNS. In case, the algorithm configured is Round Robin at the Route 53 level, then browser- 1 will get &lt;strong&gt;EIP-1(50.19.82.183&lt;/strong&gt;) of HAProxy-1 as the primary IP and browser -2 will get &lt;strong&gt;EIP-2&lt;/strong&gt; &lt;strong&gt;(23.23.174.254)&lt;/strong&gt; of HAProxy-2 as the primary IP in rotation basis. Â The browser- 1 will contact the HAProxy-1 and in case HAProxy-1 is not reachable it will contact the secondary EIP which is HAproxy-2 and so forth. This is an age old technique generally used by search engines, content servers (or) web scale systems for achieving scalability in LB layer. But this method does not provide any means of High availability @ LB layer. It requires additional measures to permanently check the HAProxy EC2 LB instances status and switch a failed EC2 instance EIP to another HAProxy EC2 LB. For this reason, this pattern is generally used as a complementary solution in High Availability, not as a primary one. Â For achieving better stability at this layer in AWS, I usually recommend having 2 or more HAProxies distributed on multiple AZâ€™s inside the Amazon EC2 region. This way if one of the HAProxy is down, the website still functions with the help of other HAProxies and even if the entire Amazon EC2 AZ is down still the HAProxies in the other AZ can handle the requests and keep the website active. Some load tests have proven that HAProxy on m1.large EC2 instance can handle close to ~4500+ HTTP requests/second. So depending upon the number of concurrent requests/sec needed on your application you can go ahead and attach multiple HAProxy EC2 instances to the Route 53. Now that we achieved availability horizontally using the Route53 DNS Round Robin in HAProxy layer let us try to understand the intricacies behind this architecture. Â Since we now have 2 or more HAProxies what will happen to the contextual web sessions data that resides in the application servers. HAProxies need to know in which application server the session data of the user resides else the requests will have authorization failures.&lt;/p&gt;

&lt;p&gt;There are 2 architecture designs we can follow for solving this contextual problem they are:&lt;/p&gt;

&lt;p&gt;**Stateless Application design: **Â This is the recommended and widely used design. The web session data is separated out from the Web/App server memory and they are kept in common cache stores like MemCacheD, TerraCotta etc.Since the session data is now kept in a common store like MemCacheD, HAProxies can direct their requests to any of the web/app servers attached under it without knowing where the session state is mapped. Whenever any web/app server receives the request from any of the HAProxies, it will validate and authorize the session data from the common store. Â In event of any HAProxy or Web/App EC2 failure still the website functions without problems because other HAProxies and Web/App servers are still able to handle the subsequent requests. Thus we achieve availability and scalability on the HAProxy/Load Balancing layer following this model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sticky Application design:&lt;/strong&gt; Things are usually not ideal and the way we assume to be in real world. Some applications are still designed with stateful nature and they store the session data, cache data etc. in their web/app server memory. Â We can always recommend the application teams to re-architect this model to stateless, but not always this suggestion works for short term migrations, inter dependencies etc.Â  So as architects we need to find way to live and cope up with this design and still try to achieve availability on the load balancing layer. HAProxy follows a technique called as â€œ&lt;strong&gt;Cookie Learningâ€&lt;/strong&gt; and &lt;strong&gt;â€œCookie Insertionâ€&lt;/strong&gt; to help state full applications. HAProxy can be configured to learn the application cookie (&amp;ldquo;JSESSIONID&amp;rdquo;), when HAProxy receives the user&amp;rsquo;s request, it checks if it contains this particular cookie and a known value. If this is not the case, it will direct the request to any Web/App EC2 server, according to the load balancing algorithm configured. HAProxy will then extract the cookie value from the response and add it along with the server&amp;rsquo;s identifier to a local table. When the user request comes back again, the load balancer sees the cookie, lookups the table and finds the Web/App EC2 server to which it forwards the request. Let me detail this important flow a little bit;&lt;/p&gt;

&lt;p&gt;HAProxy-1 EC2 instance will receive clientâ€™s requests from the browser. If a request does not contain a cookie, it will be forwarded to a valid Web/App EC2 Instance Apache-A. In return, if a JESSIONID cookie is seen, the Web/App EC2 Instance name (Example â€œAâ€)will be prefixed into it, followed by a delimiter (&amp;rsquo;~&amp;lsquo;) like &amp;ldquo;JSESSIONID=A~xxx&amp;rdquo;.When the browser client requests again with the cookieÂ  JSESSIONID=A~xxx&amp;rdquo;, HAProxy-1 will know that it must be forwarded to Web/App Instance Apache-A. The EC2 Instance name â€Aâ€ will then be extracted from cookie before it is sent to the Web/App EC2 Instance Apache-A.&lt;/p&gt;

&lt;p&gt;If Web/App EC2 Instance Apache-A dies, then requests will be sent to another valid server Web/App EC2 Instance Apache-B by LB and the cookie will be reassigned.&lt;/p&gt;

&lt;p&gt;If HAProxy-1 itself dies, then requests will be sent to HAProxy-2 which will identify the Web/App EC2 instance to forward the request. This way even if the subsequent requests moves from HAProxy-1 to HAProxy-2 in event of HAProxy-1 failure, still the requests are sent to the same Web/App instance Apache-A by the cookie learning/insertion mechanism of HAProxy.&lt;/p&gt;

&lt;p&gt;Sample HAProxy Settings to achieve this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;listen webfarm 192.168.1.1:80&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  mode http&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  balance roundrobin&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  cookie JSESSIONID prefix&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  option httpclose&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  option forwardfor&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  option httpchk HEAD /index.html HTTP/1.0&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  server Apache-A 192.168.1.11:80 cookie A check&amp;lt;/span&amp;gt;
&amp;lt;span style=&amp;quot;color:#0000ff;&amp;quot;&amp;gt;Â Â Â Â Â Â  server Apache-B 192.168.1.12:80 cookie B check&amp;lt;/span&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note: You can use more sophisticated DNS services like UltraDNS , DNSMadeEasy etc also in this architecture to better control the Load balancing and traffic direction at the DNS level.&lt;/p&gt;

&lt;p&gt;Pattern 2: Route 53 DNS RR + HAProxy in Active-Passive mode&lt;/p&gt;

&lt;p&gt;This is an extension of the Route53 DNS RR pattern and everything discussed in the previous pattern still applies to this context. In addition to associating HAProxies horizontally under Route53, we will build availability for every HAProxy vertically as well in this pattern. High Availability is built taking into consideration HAProxy process failure and HAProxy EC2 instance failure.&lt;/p&gt;

&lt;p&gt;2 or more HAProxies from multiple AZâ€™s are taken and they are attached with Amazon Elastic IPâ€™s. These Elastic IPâ€™s are then associated in Route 53 with DNS RR. These HAProxies are now â€œActiveâ€ and are ready to handle the user requests. For HA, another equivalent set of HAProxies are launched in the respective AZâ€™s as â€œStandbyâ€. In event of the â€œActiveâ€ HAProxy failure, the Standby HAProxy remaps to the same Amazon Elastic IP takes over the subsequent requests from the client.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/2.jpg&#34;&gt;&lt;img src=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/2.jpg&#34; alt=&#34;2&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the above diagram, there are 2 HAProxies in â€œActiveâ€ state with Elastic IPâ€™s &lt;strong&gt;50.19.82.183&lt;/strong&gt; &lt;strong&gt;&amp;amp;&lt;/strong&gt; &lt;strong&gt;23.23.174.254&lt;/strong&gt;. They are deployed across Multiple Availability Zones inside an Amazon EC2 region. Another 2 HAProxies are launched in respective AZâ€™s, but they are kept idle in â€œStandbyâ€ state. In event of &lt;strong&gt;HAProxy-1 (EIP: 50.19.82.183)&lt;/strong&gt; failure the Elastic IP is remapped to Standby HAProxy-3 in the same AZ. The remapping takes ~60 seconds and the HAPorxy-3 will be handling the subsequent requests directed by the browsers to the &lt;strong&gt;50.19.82.183&lt;/strong&gt;** IP.**&lt;/p&gt;

&lt;p&gt;Broadly there are 2 levels of failure in this pattern;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Failure @ HAProxy Process level&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Failure @ HAProxy EC2 instance level&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/3.jpg&#34;&gt;&lt;img src=&#34;https://www.vishnu-tech.com/wp-content/uploads/2013/04/3.jpg&#34; alt=&#34;3&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Â &lt;strong&gt;Failure @HAProxy Process level:&lt;/strong&gt; Â When HAProxy Process at the â€œActiveâ€ server fails; we can detect this using **KeepAliveD **and switch the Elastic IP from Active -&amp;gt; Standby. We have observed it takes ~60-120 seconds for the standby to takeover. During this time the &lt;strong&gt;particular HAProxy alone&lt;/strong&gt; will be unreachable. KeepAliveD script is configured in both the Active and standby HAProxy EC2 instance. KeepAliveD implements a set of checkers to dynamically and adaptively maintain and manage load balanced server pool according to their health. High availability is achieved by Virtual Router Redundancy Protocol &lt;a href=&#34;http://datatracker.ietf.org/wg/vrrp/&#34;&gt;VRRP&lt;/a&gt; protocol of the KeepAliveD. Since Amazon EC2 currently does not support Multicast protocol we need to configure KeepAliveD with Unicast TCP in this scenario.Â  For more details refer &lt;a href=&#34;http://www.keepalived.org/&#34;&gt;http://www.keepalived.org/&lt;/a&gt;. Mean time manually we can bring the failed HAProxy Process up and make this as the new standby.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script Name: â€œ/etc/keepalived/keepalived.confâ€&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;vrrp_script chk_haproxy {Â Â Â Â Â Â Â Â Â Â  # Requires keepalived-1.1.13&lt;/p&gt;

&lt;p&gt;script &amp;ldquo;killall -0 haproxy&amp;rdquo;Â Â Â Â  # cheaper than pidof&lt;/p&gt;

&lt;p&gt;interval 20Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  # check every 2 seconds&lt;/p&gt;

&lt;p&gt;weight 20Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â # add 2 points of prio if OK&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;vrrp_instance VI_1 {&lt;/p&gt;

&lt;p&gt;interface eth0&lt;/p&gt;

&lt;p&gt;state MASTER&lt;/p&gt;

&lt;p&gt;virtual_router_id 51&lt;/p&gt;

&lt;p&gt;ipriority 101Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  # 101 on master, 100 on backup&lt;/p&gt;

&lt;p&gt;vrrp_unicast_bind 10.215.31.4&lt;/p&gt;

&lt;p&gt;Â Â Â Â Â  #internal IP address of EC2 instance 01&lt;/p&gt;

&lt;p&gt;vrrp_unicast_peer 10.85.110.252&lt;/p&gt;

&lt;p&gt;Â  #internal IP address of EC2 instance 02&lt;/p&gt;

&lt;p&gt;notify_master &amp;ldquo;/etc/keepalived/vrrp.sh&amp;rdquo;&lt;/p&gt;

&lt;p&gt;track_script {&lt;/p&gt;

&lt;p&gt;chk_haproxy weight 20&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script Name: /etc/keepalived/vrrp.sh&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;#vrrp.sh&lt;/p&gt;

&lt;p&gt;#!/bin/bash&lt;/p&gt;

&lt;p&gt;cd /opt/aws/apitools/ec2/bin&lt;/p&gt;

&lt;p&gt;#DisAssociate EIP from this instance.&lt;/p&gt;

&lt;p&gt;./ec2-disassociate-address &amp;ndash;aws-access-key XXXXXXX â€“aws-secret-key XXXXXXXÂ  [EIP]&lt;/p&gt;

&lt;p&gt;#Mapping EIP to secondary server&lt;/p&gt;

&lt;p&gt;./ec2-associate-address &amp;ndash;aws-access-key XXXXXXXÂ  â€“aws-secret-key XXXXXXXÂ  [EIP] -i [ec2_instance_id_of_primary_or_secondary]&lt;/p&gt;

&lt;p&gt;**Failure @ HAProxy EC2 instance level: **When the Active HAProxy EC2 instance itself fails; we can detect this using **Heartbeat **and switch the Elastic IP from Active -&amp;gt; Standby. &lt;strong&gt;&amp;ldquo;&lt;a href=&#34;http://en.wikipedia.org/wiki/Heartbeat_%28program%29&#34;&gt;Heartbeat&lt;/a&gt;&amp;ldquo;&lt;/strong&gt; tool connects two servers and checks the regular &amp;ldquo;pulse&amp;rdquo; or &amp;ldquo;heartbeat&amp;rdquo; between them. The standby server takes over the work of the â€œActiveâ€ as soon as it detects an alteration in the &amp;ldquo;heartbeat&amp;rdquo; of the former. We have observed it takes ~120+ seconds for the standby to takeover. During this time the &lt;strong&gt;particular HAProxy alone&lt;/strong&gt; will be unreachable. Heartbeat has to be configured in both the Active and standby HAProxy EC2 instance. Since Amazon EC2 currently does not support Multicast protocol we need to configure Heartbeat with Unicast UDP in this scenario. Mean time manually we can bring the failed HAProxy EC2 instance up and make this as the new standby.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script Name: /etc/ha.d/ha.cf&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;logfile /var/log/ha-log&lt;/p&gt;

&lt;p&gt;logfacility local0&lt;/p&gt;

&lt;p&gt;keepalive 2&lt;/p&gt;

&lt;p&gt;deadtime 30&lt;/p&gt;

&lt;p&gt;initdead 120&lt;/p&gt;

&lt;p&gt;udpport 694&lt;/p&gt;

&lt;p&gt;ucast eth0 xx.xxx.xxx.xxa #Internal IP of EC2 instance 01&lt;/p&gt;

&lt;p&gt;ucast eth0 xx.xxx.xxx.xxb #Internal IP of EC2 instance 02&lt;/p&gt;

&lt;p&gt;auto_failback off&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Script Name: Create a script named â€œelastic_ipâ€ in both the servers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;#!/bin/bash&lt;/p&gt;

&lt;p&gt;I_ID=&amp;rdquo;[ec2_instance_id&amp;rdquo; # different for each EC2 servers.&lt;/p&gt;

&lt;p&gt;ELASTIC_IP=&amp;ldquo;X.X.X.X&amp;rdquo;&lt;/p&gt;

&lt;p&gt;case $1 in&lt;/p&gt;

&lt;p&gt;Â Â Â  start)&lt;/p&gt;

&lt;p&gt;ec2-associate-address &amp;ndash;aws-access-key XXXXX &amp;ndash;aws-secret-key XXXXX &amp;ldquo;$ELASTIC_IP&amp;rdquo; -i &amp;ldquo;$I_ID&amp;rdquo; &amp;gt; /dev/null&lt;/p&gt;

&lt;p&gt;Â Â Â Â Â Â  echo $0 started&lt;/p&gt;

&lt;p&gt;Â Â Â Â Â Â  ;;&lt;/p&gt;

&lt;p&gt;Â Â Â  stop)&lt;/p&gt;

&lt;p&gt;ec2-disassociate-address &amp;ndash;aws-access-key XXXXX &amp;ndash;aws-secret-key XXXXX &amp;ldquo;$ELASTIC_IP&amp;rdquo; &amp;gt; /dev/null&lt;/p&gt;

&lt;p&gt;Â Â Â  echo $0 stopped&lt;/p&gt;

&lt;p&gt;Â Â Â Â Â Â  ;;&lt;/p&gt;

&lt;p&gt;Â Â Â  status)&lt;/p&gt;

&lt;p&gt;ec2-describe-addresses &amp;ndash;aws-access-key XXXXX &amp;ndash;aws-secret-key XXXXX | grep &amp;ldquo;$ELASTIC_IP&amp;rdquo; | grep &amp;ldquo;$I_ID&amp;rdquo; &amp;gt; /dev/null&lt;/p&gt;

&lt;p&gt;Â Â Â  # grep will return true if this ip is mapped to this instance&lt;/p&gt;

&lt;p&gt;Â Â Â  [ $? -eq 0 ] &amp;amp;&amp;amp; echo $0 OK || echo $0 FAIL&lt;/p&gt;

&lt;p&gt;Â Â Â  ;;&lt;/p&gt;

&lt;p&gt;Â &lt;/p&gt;

&lt;p&gt;esac&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why do we need this redundancy in the HAProxy layer?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not all the times the DNS RR with LB Cookie Insertion alone is enough for ensuring availability;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; Imagine you have not automated the scalability @ Load Balancing Layer and one of your Load balancer is down. You do not want to be waked up in the middle of the night rather it is better to have a standby Load Balancer automatically replacing the failed one. Manually you can replace the faulty LB next day.&lt;/p&gt;

&lt;p&gt;**Case 2: **You have a gaming site where long running TCP sockets are established from flash gaming clients to the LB layer. You have planned the capacity of Front end Load Balancers with concurrent connections/sec. Now couple of your Load balancers are down, the new connections will be established to other running LB, but overall your site will now start performing poorly and chances are new connections are exhausted after few hours of heavy traffic. It is better to automatically detect and replace the faulty LB EC2 instance with the standby.&lt;/p&gt;

&lt;p&gt;**Case 3: **Some clients cache the IP address of the Load Balancer, Some of them have long running sticky sessions with web/app, Some hardware devices can take only IP address to push data into the Server infrastructure. Though it is suggested to resolve the IP using DNS, still in reality some use cases does not work the same way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pattern 3: Use ELB&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Do not worry about all the above patterns, just go and configure Amazon Elastic Load Balancing (ELB). For most of the use cases ELB is more than sufficient.&lt;/p&gt;

&lt;p&gt;Amazon Elastic Load Balancer can distribute incoming traffic across your Amazon EC2 instances in a single Availability Zone or multiple Availability Zones. Amazon Elastic Load Balancing automatically scales its request handling capacity in response to incoming application traffic. It can handle 20k+ concurrent requests/sec with ease.Â It enables you to achieve even greater fault tolerance in your applications, seamlessly providing the amount of load balancing capacity needed in response to incoming application traffic. Elastic Load Balancing detects unhealthy instances within a pool and automatically reroutes traffic to healthy instances until the unhealthy instances have been restored. Any faulty Load balancers in the ELB tier are automatically replaced.&lt;/p&gt;

&lt;p&gt;Though for most of the common use cases ELB is more than sufficient in AWS. There are some unique cases which demand the use of Load balancers like HAProxy, Nginx and NetScaler in our architecture in the AWS infrastructure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Install ec2-api Tools on Linux</title>
      <link>https://www.vishnu-tech.com/blog/install-ec2-api-tools-on-linux/</link>
      <pubDate>Mon, 10 Dec 2012 10:40:06 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/install-ec2-api-tools-on-linux/</guid>
      <description>&lt;p&gt;OS -&lt;strong&gt;Â Ubuntu Lucid&lt;/strong&gt;
ProcessorÂ &lt;strong&gt;- x64&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/351&#34;&gt;Download the ec2-api tools from the amazon site&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.oracle.com/technetwork/java/index.html&#34;&gt;Downloaded jre1.7.0_x64 from the java site&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Steps to install JAVA JRE
&lt;strong&gt;#add-apt-repository ppa:sun-java-community-team/sun-java6&lt;/strong&gt;
&lt;strong&gt;#apt-get update&lt;/strong&gt;
&lt;strong&gt;#apt-cache search java* [to know the latest jre/jdk]&lt;/strong&gt;
&lt;strong&gt;#apt-get install sun-java6-jre&lt;/strong&gt;
&lt;strong&gt;java is installed into /usr/lib/jvm/java-6-sun-1.6.0.21/&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To check successful java installation
&lt;strong&gt;#java -version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Setting up variables
&lt;strong&gt;#export JAVA_HOME=/usr/lib/jvm/java-6-sun-1.6.0.21&lt;/strong&gt;
[put it in /etc/profiles]&lt;/p&gt;

&lt;p&gt;Another Check
&lt;strong&gt;$JAVA_HOME/bin/java -version&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now the prequisites are done . So comming back to the ec2-api tools
&lt;strong&gt;#export EC2_HOME=/usr/local/ec2-api-tools-1.5.5.0&lt;/strong&gt;
[path where i unzipped it , also mention this in /etc/profile]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;#export PATH=$PATH:$EC2_HOME/bin&lt;/strong&gt;
[put it in /etc/profile too]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;#export EC2_PRIVATE_KEY=/EC2_API_Certs/pk-47O.pem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;#export EC2_CERT=/EC2_API_Certs/cert-4GV.pem&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>To Terminate an EC2 Instance, disabled via ec2-api</title>
      <link>https://www.vishnu-tech.com/blog/to-terminate-an-ec2-instance-disabled-via-ec2-api/</link>
      <pubDate>Mon, 10 Dec 2012 10:38:44 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/to-terminate-an-ec2-instance-disabled-via-ec2-api/</guid>
      <description>&lt;p&gt;#&lt;strong&gt;ec2minatt i-555555 â€“disable-api-termination false â€“region us-east-1 -K /EC2_API_Certs/pk-4GGV.pem -C /EC2_API_Certs/cert-GV.pem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Make a note of the ebs-id attached&lt;/p&gt;

&lt;p&gt;Terminate the Instance from the AWS Console and then Delete the EBS, same could be done through command line&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quick MySQL Commands</title>
      <link>https://www.vishnu-tech.com/blog/quick-mysql-commands/</link>
      <pubDate>Mon, 10 Dec 2012 10:33:27 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/quick-mysql-commands/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;To Login into the MySQL&lt;/strong&gt;
#mysql -u username -ppaswword
&lt;strong&gt;and if its RDS :&lt;/strong&gt;
#mysql -h rds.indexpoint/dnsname/ip -u username -ppassword&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To List Databases&lt;/strong&gt;
mysql&amp;gt;show databases;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To View tables&lt;/strong&gt;
mysql&amp;gt;use databasename;
mysql&amp;gt;show tables;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To View Contents Inside the Table&lt;/strong&gt;
mysql&amp;gt;select * from tablename;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Delete Database&lt;/strong&gt;
mysql&amp;gt;drop database dbname;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To create Database user (we also need to grant permission to a db to gain access for the new user.)&lt;/strong&gt;
mysql&amp;gt;create user â€˜UNNIâ€™@&amp;lsquo;ipaddress/%/localhostâ€™ identified by â€˜passwd3@1â€²;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Grant Permission to a Database for a User and Create the User at the same Time&lt;/strong&gt;
mysql&amp;gt;grant all on databasename.* to â€˜UNNIâ€™@&amp;lsquo;ipaddress/%/localhostâ€™ identified by â€˜passwordâ€™;&lt;/p&gt;

&lt;p&gt;[NOTE&amp;ndash; To provide access from all IP Address use â€˜%â€™ instead of ipaddress.]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To List out all Database Users&lt;/strong&gt;
mysql&amp;gt;SELECT user,host FROM mysql.user;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Create a Database&lt;/strong&gt;
mysql&amp;gt;create database UNNI;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To restore a specific Database in Mysql (database UNNI has to be created already)&lt;/strong&gt;
#mysql -u username -ppassword UNNI &amp;lt; UNNI.sql&lt;/p&gt;

&lt;p&gt;[NOTE: Database UNNI had to be created before restore]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To dump/backup a specific Database in Mysql&lt;/strong&gt;
#mysqldump -u username -ppassword UNNI &amp;gt; UNNI.sql&lt;/p&gt;

&lt;p&gt;[NOTE: Database UNNI is backed up into UNNI.sql]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To know the permission of Mysql User&lt;/strong&gt;
#show grants for â€˜unniâ€™@&amp;lsquo;localhostâ€™;&lt;/p&gt;

&lt;h2 id=&#34;need-to-know-create-delete-user&#34;&gt;[NEED TO KNOW]- Create/Delete User :::&lt;/h2&gt;

&lt;p&gt;It must be noted that CREATE USER command was added in the MySQL version 5.0.2. In earlier versions, users could be created automatically when assigning permissions using the GRANT command or by manually inserting records in the mysql database.&lt;/p&gt;

&lt;p&gt;The mysql database contains three tables â€“Â &lt;strong&gt;user, host and db.Â &lt;/strong&gt;These tables contains the database permissions.&lt;/p&gt;

&lt;p&gt;The user table contains the usernames and password combination of anyone who has access to any part of the MYSQL database. The password part is the encrypted string, which can be generated using the PASSWORD() function.&lt;/p&gt;

&lt;p&gt;As an administrator, you can even directly insert the values into the user table of mysql database and get the desired results.
&lt;strong&gt;&lt;code&gt;mysql&amp;gt;INSERT INTO user(Host,User,Password) VALUES(&#39;localhost&#39;, &#39;UNNI&#39;, PASSWORD(&#39;passwd1@3&#39;));
&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;mysql&amp;gt;FLUSH PRIVILEGES;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The FLUSH PRIVILEGES command is required to inform MySQL to reload the privilege data after the change is made.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deleting Users&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To delete users from the MySQL database use the DROP command.
&lt;code&gt;**mysql&amp;gt;DROP USER user@host;**&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The command in turn removes the user record from the mysql.user table.&lt;/p&gt;

&lt;p&gt;As the CREATE USER command, even the DROP USER command has been added since MySQL 5.0.2. In previous versions of MySQL you must revoke the userâ€™s privileges first, delete the records from user manually and then issue the FLUSH PRIVILEGES command.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;**mysql&amp;gt;DELETE FROM user WHERE User= &#39;technofriends&#39; AND Host= &#39;localhost&#39;;
FLUSH PRIVILEGES;**&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;General Notes&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There is no concept in MySQL of â€œOwnerâ€ of database or its objects, as there is in MS Access and MS SQL Server. I surmise this from the lack of â€œownerâ€ field anywhere in mysql system tables.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to Reset Mysql ROOT Password</title>
      <link>https://www.vishnu-tech.com/blog/how-to-reset-mysql-root-password/</link>
      <pubDate>Mon, 10 Dec 2012 10:29:59 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/how-to-reset-mysql-root-password/</guid>
      <description>&lt;p&gt;1)Â &lt;strong&gt;Stop the mysql demon&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;/etc/init.d/mysql stop&lt;/p&gt;

&lt;p&gt;2)Â &lt;strong&gt;Start the mysqld demon process using the â€“skip-grant-tables option with this command&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;/usr/sbin/mysqld â€“skip-grant-tables â€“skip-networking &amp;amp;&lt;/p&gt;

&lt;p&gt;3)&lt;strong&gt;Â start the mysql client process using this command&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;mysql -u root&lt;/p&gt;

&lt;p&gt;4)Â &lt;strong&gt;from the mysql prompt execute this command to be able to change any password&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;FLUSH PRIVILEGES;&lt;/p&gt;

&lt;p&gt;5)&lt;strong&gt;Â Then reset/update your password&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SET PASSWORD FOR root@â€™localhostâ€™ = PASSWORD(â€˜passwordâ€™);&lt;/p&gt;

&lt;p&gt;FLUSH PRIVILEGES;&lt;/p&gt;

&lt;p&gt;6)Â &lt;strong&gt;Then stop the mysqld process and relaunch it with the classical way:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;/etc/init.d/mysql stop
/etc/init.d/mysql start&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mysql- Database Sharding</title>
      <link>https://www.vishnu-tech.com/blog/mysql-database-sharding/</link>
      <pubDate>Mon, 10 Dec 2012 10:26:20 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/mysql-database-sharding/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;â€œshare nonthingâ€ : Key Law on database sharding Architecture.&lt;/strong&gt;&lt;/em&gt;
&lt;em&gt;&lt;strong&gt;Small Databases are Fast, Big Databases are Slow !!!&lt;/strong&gt;&lt;/em&gt;
&lt;em&gt;&lt;strong&gt;DB Sharding â€“ Breaking a Bigger DB into a Smaller DB.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Key Points on DB Sharding â€“&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Partition Data across master&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Writes and read are distributed&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Application needs modification&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Needs choice partitioning strategy for uniform data distribution.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Issues -&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Joins cannot be performed across shards&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Application modification can be expensive.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Example : Evernote uses database sharding â€“ localized failures , no need for joins. Each shards handles allÂ  data &amp;amp;Â  traffic for about 100,000 users.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sharding is another way to resolve MySQL scalability issues. It usually means splitting up the data by some logic derived from the application. This can be done by selecting a key in the data and splitting the data by hashing that key and having some distribution logic. It can also be done by identifying the application needs and setting different tables or different data sets in different databases (splitting the North-America sales data from the EMEA sales data, etc.)&lt;/p&gt;

&lt;p&gt;This approach is simple from the database standpoint, but is very complex from the application standpoint since the application needs to be modified to deal with the data being scattered into the different shards. Moreover, combining data from different shards can be very complex and involves development in the application (you canâ€™t just run a simple JOIN.)&lt;/p&gt;

&lt;p&gt;Advantages of scaling out and in using sharding:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Scales beyond the limitations of a single machine&lt;/li&gt;
&lt;li&gt;Scales both read and write operations (but makes some operations impossible to achieve in the database)&lt;/li&gt;
&lt;li&gt;Scales both throughput and capacity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Disadvantages of scaling out and in using sharding:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Complex and requires application changes&lt;/li&gt;
&lt;li&gt;Scaling is usually offline and requires a re-partitioning event â€“ and may require application changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Today, there are some solutions that introduce auto-sharding (Scalebase, Dbshards). This approach makes sharding more similar to shared-nothing partitioning, thus taking the sting out of some sharding complexities. However, it still requires application awareness and could prove to be a limiting factor if you needed to update your app or migrate to a different database solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Install &amp; Use s3cmd for S3 Storage</title>
      <link>https://www.vishnu-tech.com/blog/install-use-s3cmd-for-s3-storage/</link>
      <pubDate>Mon, 10 Dec 2012 10:23:24 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/install-use-s3cmd-for-s3-storage/</guid>
      <description>&lt;p&gt;Amazon S3 is a reasonably priced data storage service. Ideal for off-site backups, archiving and other data storage needs. It is generally more reliable than your regular web hosting for storing your files and images. Check out About Amazon S3 section to find out more.&lt;/p&gt;

&lt;p&gt;S3cmd is a command line tool for uploading, retrieving and managing data in Amazon S3. It is best suited for power users who donâ€™t fear command line. It is also ideal for scripts, automated backups triggered from cron, etc.&lt;/p&gt;

&lt;p&gt;S3cmd is an open source project available under GNU Public License v2 (GPLv2) and is free for both commercial and private use. You will only have to pay Amazon for using their storage. None of these money go to S3cmd developers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;#apt-get install s3cmd&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To configure s3cmd&lt;/strong&gt;
#s3cmd â€“configure
[Enter Access Key and Secret Key]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration file is saved into&lt;/strong&gt;
/root/.s3cfg&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To get Help&lt;/strong&gt;
#s3cmd â€“help&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To List Buckets&lt;/strong&gt;
#s3cmd ls&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To Delete Non-Empty Buckets&lt;/strong&gt;
#s3cmd rb s3://buckt_name -fv&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Copy buckets to local machine&lt;/strong&gt;
#s3cmd get s3://buckt_name -r&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create Buckets&lt;/strong&gt;
#s3cmd mb s3://buckt_name&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Syncing local dir with s3 Buckets&lt;/strong&gt;
#s3cmd sync local_dir/ s3://buckt_name&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AWS Products/Solutions â€“ Admins Capsule</title>
      <link>https://www.vishnu-tech.com/blog/aws-productssolutions-admins-capsule/</link>
      <pubDate>Mon, 10 Dec 2012 10:21:57 +0000</pubDate>
      
      <guid>https://www.vishnu-tech.com/blog/aws-productssolutions-admins-capsule/</guid>
      <description>

&lt;h1 id=&#34;database&#34;&gt;Database&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/running_databases/#rds&#34;&gt;Amazon RDS&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can think of aÂ &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Welcome.html&#34;&gt;RDS&lt;/a&gt;Â DB Instance as a database environment in the cloud with the compute and storage resources you specify.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can create and delete DB Instances, define/refine infrastructure attributes of your DB Instance(s), and control access and security via theÂ &lt;a href=&#34;https://console.aws.amazon.com/&#34;&gt;AWS Management Console&lt;/a&gt;, Amazon RDS APIs, and Command Line Tools.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multiple MySQL databases or SQL Server databases (up to 30) or Oracle database schemas can be created on a given DB Instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For optionalÂ &lt;a href=&#34;http://aws.amazon.com/rds/faqs/#36&#34;&gt;Multi-AZ deployments&lt;/a&gt;Â (currently supported for MySQL and Oracle database engines), Amazon RDS also manages synchronous data replication across Availability Zones and automatic failover.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/rds/faqs/&#34;&gt;Amazon RDS FAQs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By default, customers are allowed to have up to a total of 20 Amazon RDS DB instances.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/articles/2936?_encoding=UTF8&amp;amp;jiveRedirect=1&#34;&gt;RDS cannot remove storage once it has been allocated. The only way to reduce the amount of storage allocated to a DB Instance is to dump the data out of the DB Instance, create a new DB Instance with less storage space, and load the data into the new DB Instance.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Unlike Multi-AZ deployments, Read Replicas use MySQLâ€™s built-in replication and are subject to its strengths and limitations.This means recent database updates made to a standard (non Multi-AZ) source DB Instance may not be present on associated Read Replicas in the event of an unplanned outage on the source DB Instance. As such, Read Replicas do not offer the same data durability benefits as Multi-AZ deployments. While Read Replicas can provide some read availability benefits, they and are not designed to improve write availability. Read Replicas are currently supported for Amazon RDS for MySQL. They can also be used for serving read traffic when the primary database is unavailable.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The read replica mechanism uses MySQLâ€™s native, asynchronous replication. This means replicas might be lagging behind the master as they try to catch up with writes. The interesting thing about this is that multi-AZ RDS instances apparently use another,_Â proprietary type_Â of synchronous replication.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A Read Replica will stay active and continue accepting read traffic even after its corresponding source DB Instance has been deleted. If you desire to delete the Read Replica in addition to the source DB Instance, you must explicitly delete the Read Replica using the DeleteDBInstance API or AWS Management Console.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;By default and at no additional charge, Amazon RDS enables automated backups of your DB Instance with a 1 day retention period.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;During the backup window, storage I/O may be suspended while your data is being backed up. This I/O suspension typically lasts a few minutes at most. This I/O suspension is avoided with Multi-AZ DB deployments, since the backup is taken from the standby.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon RDS DB snapshots and automated backups are stored in S3.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If you desire to turn off automated backups altogether, you can do so by setting the retention period to 0 (not recommended).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;When you delete a DB Instance, you have the ability to specify whether a final DB Snapshot is created upon deletion, which enables a DB Snapshot restore of the deleted database instance at a later date. All previously created DB Snapshots of your DB Instance will be retained and billed at $0.15 per GB-month, unless you choose to delete them.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon RDS does not currently provide access to the binary logs for your Database Instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You are not charged for the data transfer incurred in replicating data between your source DB Instance and Read Replica. Billing for a Read Replica begins as soon as the Read Replica has been successfully created (i.e. when status is listed as â€œactiveâ€). The Read Replica will continue being billed at standard Amazon RDS DB Instance hour rates until you issue a command to delete it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon RDS primarily has 3 engines â€“Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Concepts.DBEngine.MySQL51.html&#34;&gt;Mysql Database Engine&lt;/a&gt;,Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Concepts.DBEngine.Oracle.html&#34;&gt;Oracle Database Engine&lt;/a&gt;,&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Concepts.DBEngine.SQLServer.html&#34;&gt;Microsoft SQL Server Database Engine&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/StartCLI.html&#34;&gt;Setup RDS CLI.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Concepts.DBInstance.html&#34;&gt;RDS Terminology and Concepts.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html&#34;&gt;How to Connect to RDS â€“ MySQL.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/CommandLineReference/command-reference.html&#34;&gt;RDS CLI â€“ References&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Scenarios.InstanceBasics.html&#34;&gt;Creating and ModifyingÂ  DB Instance.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/Scenarios.Backups.html&#34;&gt;Backing UP and Restoring DB Instances.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_ListEvents.html&#34;&gt;Viewing RDS Instance events.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;working withÂ Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html&#34;&gt;DB Parameter Groups&lt;/a&gt;,Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html&#34;&gt;security groups&lt;/a&gt;,Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_WorkingWithOptionGroups.html&#34;&gt;option groups&lt;/a&gt;Â &amp;amp;Â &lt;a href=&#34;http://docs.amazonwebservices.com/AmazonRDS/latest/UserGuide/USER_Monitoring.html&#34;&gt;viewing DB instance metrics&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/thread.jspa?messageID=161973&amp;amp;#161973&#34;&gt;RDS Security Best Practices !!!&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/thread.jspa?messageID=180752&amp;amp;#180752&#34;&gt;Tech Tips â€“ Scaling Databases with Amazon RDS&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/thread.jspa?messageID=160936&amp;amp;#160936&#34;&gt;Tech Tips â€“ On Demand Test Databases&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/thread.jspa?messageID=198765&amp;amp;#198765&#34;&gt;Tech Tips IV: Best Practices to Avoid an Inoperable RDS MySQL DB Instance&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://forums.aws.amazon.com/thread.jspa?threadID=56871&amp;amp;tstart=0&#34;&gt;Tech Tips V: Defining CloudWatch alarms for Amazon RDS metrics&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The default storage engine with RDS is InnoDB, but you are free to choose another, like the popular MyISAM. It is important to realize that read replicas on nontransactional storage engines (like MyISAM) require you to freeze your databases, as the consistency cannot be guaranteed when snapshotting. But if you use InnoDB, you are safe, and the only thing you have to do is fire up a new read replica.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RDS storage is independent of RDS instance classes. Every class can have from 5 GB to 1 TB of storage associated. Scaling up the storage is easy, and you can do it using the Console. It does require a reboot. On the other hand, scaling down the storage is impossible.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/rds/reserved-instances/&#34;&gt;Reserved DB Instances page&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/rds/&#34;&gt;On-Demand DB Instances.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/2928/&#34;&gt;RDS CLI&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://blog.webyog.com/2009/11/16/top-10-things-to-know-about-amazon-rds/&#34;&gt;10 things you should know about RDS.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/running_databases/#dynamodb&#34;&gt;Amazon DynamoDB&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A fast, highly scalable NoSQL database service&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A fully managed service that offers extremely fast performance, seamless scalability and reliability, low cost and more.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.youtube.com/watch?v=oz-7wJJ9HZ0&#34;&gt;Video&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/running_databases/#simpledb&#34;&gt;Amazon SimpleDB&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A NoSQL database service for smaller datasets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A fully managed service that provides a schemaless database, reliability and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/running_databases/#relational_amis&#34;&gt;Your choice of relational AMIs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A relational database you can manage on your own.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;On Amazon EC2 and EBS that provide scale compute &amp;amp; storage, complete control over instances, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/elasticache/&#34;&gt;Amazon ElastiCache&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/2310261897259567&#34;&gt;Amazon ElasticCache CLI Tool.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;compute&#34;&gt;Compute&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/ec2/&#34;&gt;Amazon Elastic Cloud Compute (EC2)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Amazon Elastic Compute Cloud delivers scalable, pay-as-you-go compute capacity in the cloud.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/ec2/instance-types&#34;&gt;Amazon EC2 Instance Types&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;EBS-Optimized instances are for selected types only such as â€“ Standard Instances(Large,Extra Large), High-Memory Instances(High-Memory Quadruple Extra Large).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Amazon EC2 instances are grouped into seven families: Standard, Micro, High-Memory, High-CPU, Cluster Compute, Cluster GPU, and High I/O.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/351&#34;&gt;Amazon EC2 API (CLI Tools)&lt;/a&gt;Â andÂ &lt;a href=&#34;http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/SettingUp_CommandLine.html&#34;&gt;how to setup&lt;/a&gt;Â it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/739&#34;&gt;Simple CLI Access to Amazon EC2 and S3&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/elasticmapreduce/&#34;&gt;Amazon Elastic MapReduce&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon Elastic MapReduce is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://aws.amazon.com/autoscaling/&#34;&gt;Auto Scaling&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Auto Scaling allows you to automatically scale your Amazon EC2 capacity up or down according to conditions you define.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/2535&#34;&gt;Auto Scaling CLI Tool.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;networking&#34;&gt;Networking&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/elasticloadbalancing/&#34;&gt;&lt;strong&gt;Elastic Load Balancing&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://aws.amazon.com/developertools/2536&#34;&gt;ELB API Tools&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Elastic Load Balancing automatically distributes incoming application traffic across multiple Amazon EC2 instances.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>